{"cells":[{"cell_type":"markdown","source":["###1. Random Data Generation\nRandom data generation is useful for testing of existing algorithms and implementing randomized algorithms, such as random projection."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"acbb25e2-b718-4f5c-b31f-de1644511ecd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import rand, randn"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5bc67c7-a090-47a2-9338-7aa1b93efe57","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###2. Summary and Descriptive Statistics\nThe function describe returns a DataFrame containing information such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"176a5e5e-3964-4079-9eea-1eba22439eed","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import rand, randn\n# A slightly different way to generate the two random columns\ndf = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10)).withColumn('normal', randn(seed=27))\ndf.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fcb8a41c-6a1e-42bc-8233-57882022fb95","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------------------+-------------------+-------------------+\n|summary|                id|            uniform|             normal|\n+-------+------------------+-------------------+-------------------+\n|  count|                10|                 10|                 10|\n|   mean|               4.5| 0.3865003272265095| 0.4175292757722803|\n| stddev|3.0276503540974917|0.34583270415314343| 0.8931883269570855|\n|    min|                 0|0.03422639313807285|-1.0451987154313813|\n|    max|                 9| 0.9899129399827472| 1.7264843633887004|\n+-------+------------------+-------------------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["If you have a DataFrame with a large number of columns, you can also run describe on a subset of the columns:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfed1205-8d2c-4b60-8ec7-9e65ad6b4ed6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.describe('uniform', 'normal').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"136cd30d-4c1d-49a8-85e5-d8860bd33fed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-------------------+-------------------+\n|summary|            uniform|             normal|\n+-------+-------------------+-------------------+\n|  count|                 10|                 10|\n|   mean| 0.3865003272265095| 0.4175292757722803|\n| stddev|0.34583270415314343| 0.8931883269570855|\n|    min|0.03422639313807285|-1.0451987154313813|\n|    max| 0.9899129399827472| 1.7264843633887004|\n+-------+-------------------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["We can also control the list of descriptive statistics and the columns they apply to using the normal select on a DataFrame:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"759ba0fb-00dc-41d6-a95d-888653eebabf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import mean, min, max\ndf.select([mean('uniform'), min('uniform'), max('uniform')]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0c6a970-bbad-4d44-88ca-f01e8367c2a4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------+-------------------+------------------+\n|      avg(uniform)|       min(uniform)|      max(uniform)|\n+------------------+-------------------+------------------+\n|0.3865003272265095|0.03422639313807285|0.9899129399827472|\n+------------------+-------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###3. Sample covariance and correlation\nCovariance is a measure of how two variables change with respect to each other."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51cec62e-6e7a-4cac-af1b-391764fde884","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import rand\ndf = sqlContext.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81f4cc40-d65b-4aba-8727-e7df52b85d83","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.stat.cov('rand1', 'rand2')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"841f2797-ab30-49fc-b853-df02d444cc0f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[8]: -0.022631283696310282"]}],"execution_count":0},{"cell_type":"code","source":["df.stat.cov('id', 'id')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8b0d5f9-7d47-493a-9348-d3c6cd2f55a8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[7]: 9.166666666666666"]}],"execution_count":0},{"cell_type":"markdown","source":["The covariance value of 9.17 might be hard to interpret. Correlation is a normalized measure of covariance that is easier to understand, as it provides quantitative measurements of the statistical dependence between two random variables."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c159058f-0509-478b-a43a-916ceb6fdafd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.stat.corr('rand1', 'rand2')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72aca8e1-1ade-4298-b1cb-e120c63125f5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[9]: -0.3600685552217083"]}],"execution_count":0},{"cell_type":"code","source":["df.stat.corr('id', 'id')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3152fda-42ae-4410-b61f-e6e8cd827284","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[10]: 1.0"]}],"execution_count":0},{"cell_type":"markdown","source":["In the above example, id correlates perfectly with itself, while the two randomly generated columns have low correlation value."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"979f0510-bed6-4437-8261-c5d960ae8d40","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###4. Cross Tabulation (Contingency Table)\nCross Tabulation provides a table of the frequency distribution for a set of variables."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73a843b5-4d52-41ec-b373-162019865909","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Create a DataFrame with two columns (name, item)\nnames = [\"Alice\", \"Bob\", \"Mike\"]\nitems = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\ndf = sqlContext.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])\n# Take a look at the first 10 rows.\ndf.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d76ae96a-4ef6-4354-bd6b-a7968e2a8b10","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-------+\n| name|   item|\n+-----+-------+\n|Alice|   milk|\n|  Bob|  bread|\n| Mike| butter|\n|Alice| apples|\n|  Bob|oranges|\n| Mike|   milk|\n|Alice|  bread|\n|  Bob| butter|\n| Mike| apples|\n|Alice|oranges|\n+-----+-------+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.stat.crosstab(\"name\", \"item\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9269cc0d-1725-4168-a3de-a7b143d45432","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+------+-----+------+----+-------+\n|name_item|apples|bread|butter|milk|oranges|\n+---------+------+-----+------+----+-------+\n|     Mike|     7|    6|     7|   7|      6|\n|    Alice|     7|    7|     6|   7|      7|\n|      Bob|     6|    7|     7|   6|      7|\n+---------+------+-----+------+----+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###5. Frequent Items\nFiguring out which items are frequent in each column can be very useful to understand a dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"08433a80-6380-4165-9014-6dfe5debc268","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = sqlContext.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\ndf.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0a8e96d-d105-4055-bb15-a872c3a8bd31","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+---+\n|  a|  b|  c|\n+---+---+---+\n|  1|  2|  3|\n|  1|  2|  1|\n|  1|  2|  3|\n|  3|  6|  3|\n|  1|  2|  3|\n|  5| 10|  1|\n|  1|  2|  3|\n|  7| 14|  3|\n|  1|  2|  3|\n|  9| 18|  1|\n+---+---+---+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4)\n###the following code finds the frequent items that show up 40% of the time for each column"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f93c9fb8-4422-4ec2-97c3-3d7988a11a3e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["freq.collect()[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f88017ca-56e4-470f-8f3d-1c068b8d08b5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[4]: Row(a_freqItems=[11, 1], b_freqItems=[2, 22], c_freqItems=[1, 3])"]}],"execution_count":0},{"cell_type":"markdown","source":["“11” and “1” are the frequent values for column “a”"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10a5f42e-0b9a-4734-b559-c0bafdbb3b78","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["we can also find frequent items for column combinations, by creating a composite column using the struct function:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4bc8fa6-d9b3-4708-9a4d-935af915f015","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import struct\n\nfreq = df.withColumn('ab', struct('a', 'b')).stat.freqItems(['ab'], 0.4)\n\nfreq.collect()[0]\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6cf90de1-d6f6-4990-a0df-dcf48dd6a26d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[5]: Row(ab_freqItems=[Row(a=11, b=22), Row(a=1, b=2)])"]}],"execution_count":0},{"cell_type":"markdown","source":["From the above, the combination of “a=11 and b=22”, and “a=1 and b=2” appear frequently in this dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b9d3606-f9ec-4ec8-b7b8-b245b52be746","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###6. Mathematical Functions\nThe inputs need to be columns functions that take a single argument, such as cos, sin, floor, ceil. For functions that take two arguments as input, such as pow, hypot, either two columns or a combination of a double and column can be supplied."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"333edf1d-d53a-40d3-ae2b-0e865b7ffef3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10) * 3.14)\n# you can reference a column or supply the column name\ndf.select(\n      'uniform',\n      toDegrees('uniform'),\n      (pow(cos(df['uniform']), 2) + pow(sin(df.uniform), 2)). \\\n        alias(\"cos^2 + sin^2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34744040-60e1-40d1-9086-71561047f518","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["/databricks/spark/python/pyspark/sql/functions.py:770: FutureWarning: Deprecated in 2.1, use degrees instead.\n  warnings.warn(\"Deprecated in 2.1, use degrees instead.\", FutureWarning)\n+-------------------+------------------+------------------+\n|            uniform|  DEGREES(uniform)|     cos^2 + sin^2|\n+-------------------+------------------+------------------+\n| 0.5367821013180484| 30.75534892368792|               1.0|\n|0.10747087445354876| 6.157627526768682|               1.0|\n| 1.1475525508626785| 65.74991793390322|               1.0|\n|  1.310955978808693|  75.1122447131799|               1.0|\n| 3.1083266315458262|178.09399733569154|0.9999999999999999|\n| 0.5165986402305565|29.598921787408106|               1.0|\n| 0.5696528438969835| 32.63870374292187|0.9999999999999999|\n| 1.5573024855692674| 89.22685984835182|0.9999999999999999|\n| 3.0450071328478523|174.46605729941354|               1.0|\n|0.23646103537894467|13.548219346507173|               1.0|\n+-------------------+------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c4de877-6c4b-4459-82ff-b32bbe01e5f9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Statistical and Mathematical Functions with DataFrames in Apache Spark (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":246377988539224}},"nbformat":4,"nbformat_minor":0}
